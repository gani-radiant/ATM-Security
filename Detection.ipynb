{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit space bar to capture\n",
    "def captureuser(name):\n",
    "    cam = cv2.VideoCapture(0)\n",
    "\n",
    "    cv2.namedWindow(\"capture\")\n",
    "\n",
    "    img_counter = 0\n",
    "    \n",
    "    dirname = f'dataset/{name}'\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        cv2.imshow(\"capture\", frame)\n",
    "        \n",
    "        if img_counter == 5:\n",
    "            cv2.destroyWindow(\"capture\")\n",
    "            break\n",
    "        if not ret:\n",
    "            break\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        elif k%256 == 32:\n",
    "            # SPACE pressed\n",
    "            path = f'dataset/{name}'\n",
    "            img_name = \"{}.jpg\".format(img_counter)\n",
    "            cv2.imwrite(os.path.join(path , img_name), frame)\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            print(\"{} written!\".format(img_name))\n",
    "            img_counter += 1\n",
    "\n",
    "    cam.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.jpg written!\n1.jpg written!\n2.jpg written!\n3.jpg written!\n4.jpg written!\n"
    }
   ],
   "source": [
    "captureuser('tundeAdewole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(unique_id,name,bank, password, account_balance):\n",
    "    import csv\n",
    "    \n",
    "    with open(r'bank_details.csv','a', newline = '\\n') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([unique_id,name,bank, password, account_balance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv('10234', 'Adewole Tunde 1', 'Gtbank', '12345', '20000.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    #summary:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # construct the argument parser and parse the arguments\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-i\", \"--dataset\", required=True,\n",
    "        help=\"path to input directory of faces + images\")\n",
    "    ap.add_argument(\"-e\", \"--embeddings\", required=True,\n",
    "        help=\"path to output serialized db of facial embeddings\")\n",
    "    ap.add_argument(\"-d\", \"--detector\", required=True,\n",
    "        help=\"path to OpenCV's deep learning face detector\")\n",
    "    ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
    "        help=\"path to OpenCV's deep learning face embedding model\")\n",
    "    ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "        help=\"minimum probability to filter weak detections\")\n",
    "    #args = vars(ap.parse_args())\n",
    "    \n",
    "    # load our serialized face detector from disk\n",
    "    print(\"[INFO] loading face detector...\")\n",
    "\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    # load our serialized face embedding model from disk\n",
    "    embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n",
    "\n",
    "    # grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying faces...\")\n",
    "    imagePaths = list(paths.list_images('dataset'))\n",
    "    # initialize our lists of extracted facial embeddings and\n",
    "    # corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "    # initialize the total number of faces processed\n",
    "    total = 0\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "            len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image, resize it to have a width of 600 pixels (while\n",
    "        # maintaining the aspect ratio), and then grab the image\n",
    "        # dimensions\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = imutils.resize(image, width=600)\n",
    "        (h, w) = image.shape[:2]\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize\n",
    "        # faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        # ensure at least one face was found\n",
    "        if len(detections) > 0:\n",
    "            # we're making the assumption that each image has only ONE\n",
    "            # face, so find the bounding box with the largest probability\n",
    "            i = np.argmax(detections[0, 0, :, 2])\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # ensure that the detection with the largest probability also\n",
    "            # means our minimum probability test (thus helping filter out\n",
    "            # weak detections)\n",
    "            if confidence > 0.5:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # extract the face ROI and grab the ROI dimensions\n",
    "                face = image[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # construct a blob for the face ROI, then pass the blob\n",
    "                # through our face embedding model to obtain the 128-d\n",
    "                # quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "    \n",
    "                # add the name of the person + corresponding face\n",
    "                # embedding to their respective lists\n",
    "                knownNames.append(name)\n",
    "                knownEmbeddings.append(vec.flatten())\n",
    "                total += 1\n",
    "    # dump the facial embeddings + names to disk\n",
    "    print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    f = open('output/embeddings.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading face detector...\n"
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6afbb6670377>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-763d5c4303c1>\u001b[0m in \u001b[0;36mget_embeddings\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromCaffe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'face_detection_model/deploy.prototxt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# load our serialized face embedding model from disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0membedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromTorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'openface_nn4.small2.v1.t7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# grab the paths to the input images in our dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n"
     ]
    }
   ],
   "source": [
    "get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"[INFO] loading face embeddings...\")\n",
    "    data = pickle.loads(open('output/embeddings.pickle', \"rb\").read())\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(data[\"names\"])\n",
    "    # train the model used to accept the 128-d embeddings of the face and\n",
    "    # then produce the actual face recognition\n",
    "    print(\"[INFO] training model...\")\n",
    "    recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "    recognizer.fit(data[\"embeddings\"], labels)\n",
    "    # write the actual face recognition model to disk\n",
    "    f = open('output/recognizer.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(recognizer))\n",
    "    f.close()\n",
    "\n",
    "    # write the label encoder to disk\n",
    "    f = open('output/le.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(le))\n",
    "    f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading face embeddings...\n[INFO] training model...\n"
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_check():\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # load our serialized face embedding model from disk\n",
    "    print(\"[INFO] loading face recognizer...\")\n",
    "    embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n",
    "\n",
    "    # load the actual face recognition model along with the label encoder\n",
    "    recognizer = pickle.loads(open('output/recognizer.pickle', \"rb\").read())\n",
    "    le = pickle.loads(open('output/le.pickle', \"rb\").read())\n",
    "\n",
    "    # initialize the video stream, then allow the camera sensor to warm up\n",
    "    print(\"[INFO] starting video stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    #run check for only 15seconds and then stop\n",
    "    timeout = time.time() + 15\n",
    "    \n",
    "    # start the FPS throughput estimator\n",
    "    fps = FPS().start()\n",
    "\n",
    "        # loop over frames from the video file stream\n",
    "    while True:\n",
    "        #run check for only 15seconds and then stop\n",
    "        if time.time() > timeout:\n",
    "            cv2.destroyWindow(\"Frame\")\n",
    "            break\n",
    "            \n",
    "        # grab the frame from the threaded video stream\n",
    "        frame = vs.read()\n",
    "\n",
    "        # resize the frame to have a width of 600 pixels (while\n",
    "        # maintaining the aspect ratio), and then grab the image\n",
    "        # dimensions\n",
    "        frame = imutils.resize(frame, width=600)\n",
    "        (h, w) = frame.shape[:2]\n",
    "\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize\n",
    "        # faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        #TODO: if 2 faces are detected alert the user of a warning\n",
    "        # loop over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with\n",
    "            # the prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # filter out weak detections\n",
    "            if confidence > 0.5:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # extract the face ROI\n",
    "                face = frame[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # construct a blob for the face ROI, then pass the blob\n",
    "                # through our face embedding model to obtain the 128-d\n",
    "                # quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "\n",
    "                # perform classification to recognize the face\n",
    "                preds = recognizer.predict_proba(vec)[0]\n",
    "                j = np.argmax(preds)\n",
    "                proba = preds[j]\n",
    "                name = le.classes_[j]\n",
    "\n",
    "                # draw the bounding box of the face along with the\n",
    "                # associated probability\n",
    "                text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "                y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                    (0, 0, 255), 2)\n",
    "                cv2.putText(frame, text, (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        # update the FPS counter\n",
    "        fps.update()\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "    fps.stop()\n",
    "    print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "    print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "    vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading face recognizer...\n"
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e77d4c5dcbf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvideo_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-75b1bf46cd89>\u001b[0m in \u001b[0;36mvideo_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# load our serialized face embedding model from disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] loading face recognizer...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0membedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromTorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'openface_nn4.small2.v1.t7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# load the actual face recognition model along with the label encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n"
     ]
    }
   ],
   "source": [
    "video_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our serialized face detector from disk\n",
    "def recognize(imagePath):\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"[INFO] loading face detector...\")\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    \n",
    " \n",
    "    # load our serialized face embedding model from disk\n",
    "    print(\"[INFO] loading face recognizer...\")\n",
    "    embedder = cv2.dnn.readNetFromTorch('nn4.small2.v1.t7')\n",
    " \n",
    "    # load the actual face recognition model along with the label encoder\n",
    "    recognizer = pickle.loads(open('output/recognizer.pickle', \"rb\").read())\n",
    "    le = pickle.loads(open('output/le.pickle', \"rb\").read())\n",
    "    \n",
    "    # load the image, resize it to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    " \n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    " \n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    " \n",
    "        # filter out weak detections\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    " \n",
    "            # extract the face ROI\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    " \n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "                (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    " \n",
    "            # perform classification to recognize the face\n",
    "            preds = recognizer.predict_proba(vec)[0]\n",
    "            j = np.argmax(preds)\n",
    "            proba = preds[j]\n",
    "            name = le.classes_[j]\n",
    "            \n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            \n",
    "            #TODO: Handle if 2 faces are given.\n",
    "            #Decision boundary\n",
    "            if (name != 'unknown') and (proba *100) < 50:\n",
    "                print(\"Fraud detected\")\n",
    "            else:\n",
    "                print(name)\n",
    "            cv2.putText(image, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    " \n",
    "    # show the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading face detector...\n[INFO] loading face recognizer...\n"
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e1e11bdd17d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecognize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/trisha_adrian.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-ce19dc67db2c>\u001b[0m in \u001b[0;36mrecognize\u001b[1;34m(imagePath)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# load our serialized face embedding model from disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] loading face recognizer...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0membedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromTorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'openface_nn4.small2.v1.t7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# load the actual face recognition model along with the label encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\THDiskFile.cpp:496: error: (-2:Unspecified error) cannot open <openface_nn4.small2.v1.t7> in mode r  in function 'TH::THDiskFile_new'\n"
     ]
    }
   ],
   "source": [
    "recognize('images/trisha_adrian.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-24-a5d9c1160d32>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-a5d9c1160d32>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def enrollment(Fullname, unique_id, password, images, account_balance):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homescreen():\n",
    "    from PIL import ImageTk, Image\n",
    "    window = tk.Tk()\n",
    "    window.title(\"WELCOME\")\n",
    "    label = tk.Label(window, text = \"Welcome to GTbank\").pack()\n",
    "    background_image = 'GUI/bg.jpg'\n",
    "    \n",
    "   \n",
    "    \n",
    "   \n",
    "\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}